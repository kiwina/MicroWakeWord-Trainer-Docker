{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6005480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Advanced MicroWakeWord Training Notebook (Docker Version)\n",
    "#\n",
    "# This notebook provides an advanced training workflow for MicroWakeWord models, with more samples, deeper models, and additional tuning options compared to the basic notebook.\n",
    "#\n",
    "# Python 3.11.11 compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa0d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CELL 1: Initial Setup, Dependency Checks, Data Preparation Call, and Path Definitions (Advanced Notebook)\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "# Check for required dependencies\n",
    "def check_dependency(package_name, min_version=None):\n",
    "    \"\"\"Check if a package is installed and optionally verify its version.\"\"\"\n",
    "    try:\n",
    "        spec = importlib.util.find_spec(package_name)\n",
    "        if spec is None:\n",
    "            return False, f\"{package_name} is not installed\"\n",
    "\n",
    "        if min_version:\n",
    "            pkg = importlib.import_module(package_name)\n",
    "            version = getattr(pkg, '__version__', '0.0.0')\n",
    "            if version < min_version:\n",
    "                return False, f\"{package_name} version {version} is installed, but version {min_version} or higher is required\"\n",
    "\n",
    "        return True, f\"{package_name} is installed\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error checking {package_name}: {str(e)}\"\n",
    "\n",
    "# List of required dependencies based on requirements.txt\n",
    "required_dependencies = [\n",
    "    \"torch\",\n",
    "    \"torchaudio\",\n",
    "    \"torchvision\",\n",
    "    \"audiomentations\",\n",
    "    \"audioread\",\n",
    "    \"librosa\",\n",
    "    \"soundfile\",\n",
    "    \"soxr\",\n",
    "    \"webrtcvad\",\n",
    "    \"datasets\",\n",
    "    \"dill\",\n",
    "    \"filelock\",\n",
    "    \"fsspec\",\n",
    "    \"huggingface_hub\",\n",
    "    \"mmap_ninja\",\n",
    "    \"multiprocess\",\n",
    "    \"pandas\",\n",
    "    \"pooch\",\n",
    "    \"pyarrow\",\n",
    "    \"xxhash\",\n",
    "    \"microwakeword\"  # This should be installed from the local project via Dockerfile\n",
    "]\n",
    "\n",
    "print(\"Checking required dependencies...\")\n",
    "missing_dependencies = []\n",
    "for dep in required_dependencies:\n",
    "    is_installed, message = check_dependency(dep)\n",
    "    print(f\"  {message}\")\n",
    "    if not is_installed:\n",
    "        missing_dependencies.append(dep)\n",
    "\n",
    "if missing_dependencies:\n",
    "    print(f\"\\nWARNING: The following dependencies are missing: {', '.join(missing_dependencies)}\")\n",
    "    print(\"Some notebook cells may fail. Please ensure all dependencies are installed.\")\n",
    "    print(\"If running in Docker, these should be installed automatically via the Dockerfile.\")\n",
    "else:\n",
    "    print(\"\\nAll required dependencies are installed.\")\n",
    "\n",
    "# Conditional import for display, works in Jupyter, no-op in pure script\n",
    "try:\n",
    "    from IPython.display import Audio, display, FileLink\n",
    "except ImportError:\n",
    "    def display(*args, **kwargs): pass\n",
    "    def Audio(*args, **kwargs): pass\n",
    "    def FileLink(*args, **kwargs): pass\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\") # Should be /data in Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec2915",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CELL 2: Initial Setup and Data Preparation (Advanced Notebook)\n",
    "# This cell runs the main data preparation script.\n",
    "# It ensures all necessary repositories and datasets are downloaded and processed into /data.\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\") # Should be /data in Docker\n",
    "\n",
    "PREPARE_DATA_SCRIPT_PATH = Path(\"/data/prepare_local_data.py\")\n",
    "DATA_DIR_INSIDE_DOCKER = Path(\"/data\") # Consistent with prepare_local_data.py's default in Docker\n",
    "\n",
    "print(f\"\\n--- Running Data Preparation Script: {PREPARE_DATA_SCRIPT_PATH} (Advanced Notebook) ---\")\n",
    "if PREPARE_DATA_SCRIPT_PATH.exists():\n",
    "    try:\n",
    "        completed_process = subprocess.run(\n",
    "            [sys.executable, str(PREPARE_DATA_SCRIPT_PATH), \"--data-dir\", str(DATA_DIR_INSIDE_DOCKER)],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        print(\"Data preparation script stdout:\")\n",
    "        print(completed_process.stdout)\n",
    "        if completed_process.stderr:\n",
    "            print(\"Data preparation script stderr:\")\n",
    "            print(completed_process.stderr)\n",
    "        print(\"Data preparation script finished successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"ERROR: Data preparation script failed with exit code {e.returncode}\")\n",
    "        print(\"Stdout:\", e.stdout)\n",
    "        print(\"Stderr:\", e.stderr)\n",
    "        sys.exit(f\"Data preparation failed (script error), cannot continue. Check logs for {PREPARE_DATA_SCRIPT_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Python executable not found at {sys.executable}\")\n",
    "        sys.exit(\"Python executable not found for data prep script.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while running prepare_local_data.py: {e}\")\n",
    "        sys.exit(f\"Unexpected error running data prep script: {e}\")\n",
    "else:\n",
    "    print(f\"ERROR: {PREPARE_DATA_SCRIPT_PATH} not found. Please ensure it's copied to /data by startup.sh.\")\n",
    "    sys.exit(f\"{PREPARE_DATA_SCRIPT_PATH} not found, critical for data setup.\")\n",
    "print(\"--- Data Preparation Finished ---\\n\")\n",
    "\n",
    "# Define target_word early as it's used in path definitions\n",
    "target_word = 'hey_norman'  # Phonetic spellings may produce better samples. User should change this.\n",
    "print(f\"Target wake word set to: {target_word}\")\n",
    "\n",
    "# Define base paths that subsequent cells will use for the ADVANCED notebook\n",
    "PIPER_SAMPLE_GENERATOR_DIR = DATA_DIR_INSIDE_DOCKER / \"piper-sample-generator\" # Shared\n",
    "PIPER_SCRIPT_PATH = PIPER_SAMPLE_GENERATOR_DIR / \"generate_samples.py\"\n",
    "\n",
    "# Updated generated samples directory structure as per feedback\n",
    "GENERATED_SAMPLES_BASE_DIR_ADV = DATA_DIR_INSIDE_DOCKER / \"generated_samples\" / target_word # Shared base for target word\n",
    "TEST_SAMPLE_OUTPUT_DIR_ADV = GENERATED_SAMPLES_BASE_DIR_ADV / \"test_adv\" # Suffix for advanced test samples\n",
    "WW_SAMPLES_OUTPUT_DIR_ADV = GENERATED_SAMPLES_BASE_DIR_ADV / \"samples_adv\" # Suffix for advanced WW samples\n",
    "\n",
    "MIT_RIRS_PATH = DATA_DIR_INSIDE_DOCKER / \"mit_rirs\" # Shared\n",
    "FMA_16K_PATH = DATA_DIR_INSIDE_DOCKER / \"fma_16k\" # Shared\n",
    "AUDIOSONET_16K_PATH = DATA_DIR_INSIDE_DOCKER / \"audioset_16k\" # Shared\n",
    "NEGATIVE_DATASETS_PATH = DATA_DIR_INSIDE_DOCKER / \"negative_datasets\" # Shared\n",
    "\n",
    "# Specific paths for advanced notebook outputs\n",
    "AUGMENTED_FEATURES_DIR_ADV = DATA_DIR_INSIDE_DOCKER / \"generated_augmented_features_adv\"\n",
    "TRAINED_MODELS_BASE_PATH_ADV = DATA_DIR_INSIDE_DOCKER / \"trained_models_adv\"\n",
    "TRAINING_CONFIG_PATH_ADV = DATA_DIR_INSIDE_DOCKER / \"training_parameters_adv.yaml\"\n",
    "\n",
    "# Ensure key directories exist\n",
    "TEST_SAMPLE_OUTPUT_DIR_ADV.mkdir(parents=True, exist_ok=True)\n",
    "WW_SAMPLES_OUTPUT_DIR_ADV.mkdir(parents=True, exist_ok=True)\n",
    "AUGMENTED_FEATURES_DIR_ADV.mkdir(parents=True, exist_ok=True)\n",
    "(TRAINED_MODELS_BASE_PATH_ADV / \"wakeword\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to check if assets exist in the data directory\n",
    "def check_assets_exist(directory, pattern=\"*\"):\n",
    "    \"\"\"Check if assets exist in the specified directory matching the pattern.\"\"\"\n",
    "    path = Path(directory)\n",
    "    if not path.exists():\n",
    "        return False, 0\n",
    "    \n",
    "    files = list(path.glob(pattern))\n",
    "    return len(files) > 0, len(files)\n",
    "\n",
    "# Ensure piper-sample-generator is in sys.path if its modules are imported directly later\n",
    "if str(PIPER_SAMPLE_GENERATOR_DIR) not in sys.path:\n",
    "    sys.path.append(str(PIPER_SAMPLE_GENERATOR_DIR))\n",
    "    print(f\"Added {PIPER_SAMPLE_GENERATOR_DIR} to sys.path\")\n",
    "\n",
    "print(\"Initial setup cell for Advanced Notebook (cell_2.py) complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dcb8c8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CELL 3: Generate Test Sample and Wake Word Samples (Advanced)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path # Ensure Path is imported\n",
    "\n",
    "# Conditional import for display\n",
    "try:\n",
    "    from IPython.display import Audio, display\n",
    "except ImportError:\n",
    "    def display(*args, **kwargs): pass\n",
    "    def Audio(*args, **kwargs): pass\n",
    "\n",
    "# Variables from cell_2.py:\n",
    "# target_word, PIPER_SCRIPT_PATH, TEST_SAMPLE_OUTPUT_DIR_ADV, WW_SAMPLES_OUTPUT_DIR_ADV\n",
    "\n",
    "if 'PIPER_SCRIPT_PATH' not in globals() or \\\n",
    "   'target_word' not in globals() or \\\n",
    "   'TEST_SAMPLE_OUTPUT_DIR_ADV' not in globals() or \\\n",
    "   'WW_SAMPLES_OUTPUT_DIR_ADV' not in globals():\n",
    "    print(\"ERROR: Essential variables not defined. Ensure cell_2.py ran correctly.\")\n",
    "else:\n",
    "    # Create directories if they don't exist (idempotent)\n",
    "    TEST_SAMPLE_OUTPUT_DIR_ADV.mkdir(parents=True, exist_ok=True)\n",
    "    WW_SAMPLES_OUTPUT_DIR_ADV.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not PIPER_SCRIPT_PATH.exists():\n",
    "        print(f\"ERROR: Piper sample generator script not found at {PIPER_SCRIPT_PATH}. Check data preparation in cell_2.\")\n",
    "    else:\n",
    "        # 1. Generate 1 test sample of the target word for manual verification.\n",
    "        print(f\"\\n--- Generating Test Sample for '{target_word}' (Advanced) ---\")\n",
    "        \n",
    "        # Check if test sample already exists (asset caching)\n",
    "        assets_exist, file_count = check_assets_exist(TEST_SAMPLE_OUTPUT_DIR_ADV, \"*.wav\")\n",
    "        if assets_exist:\n",
    "            print(f\"Found {file_count} existing test samples in {TEST_SAMPLE_OUTPUT_DIR_ADV}\")\n",
    "            audio_path_test_adv = next(TEST_SAMPLE_OUTPUT_DIR_ADV.glob(\"*.wav\"))\n",
    "            print(f\"Using existing test sample: {audio_path_test_adv}\")\n",
    "            display(Audio(str(audio_path_test_adv), autoplay=True))\n",
    "        else:\n",
    "            test_sample_cmd = f\"\\\"{sys.executable}\\\" \\\"{str(PIPER_SCRIPT_PATH)}\\\" \\\"{target_word}\\\" \\\\\n",
    "            --max-samples 1 \\\\\n",
    "            --batch-size 1 \\\\\n",
    "            --output-dir \\\"{str(TEST_SAMPLE_OUTPUT_DIR_ADV)}\\\"\"\n",
    "            \n",
    "            print(f\"Executing: {test_sample_cmd}\")\n",
    "            try:\n",
    "                if \"get_ipython\" in globals():\n",
    "                    get_ipython().system(test_sample_cmd)\n",
    "                else:\n",
    "                    print(\"Warning: get_ipython() not available. Cannot execute command directly. For .py, use subprocess.\")\n",
    "                    # import subprocess\n",
    "                    # subprocess.run(test_sample_cmd, shell=True, check=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing sample generation command: {e}\")\n",
    "                print(\"This might be due to missing dependencies or configuration issues.\")\n",
    "\n",
    "            audio_path_test_adv = TEST_SAMPLE_OUTPUT_DIR_ADV / \"0.wav\"\n",
    "            if audio_path_test_adv.exists():\n",
    "                print(f\"Playing test sample: {audio_path_test_adv}\")\n",
    "                display(Audio(str(audio_path_test_adv), autoplay=True))\n",
    "            else:\n",
    "                print(f\"Audio file not found at {audio_path_test_adv}. Sample generation might have failed.\")\n",
    "\n",
    "        # 2. Generates a larger amount of wake word samples for advanced training.\n",
    "        print(f\"\\n--- Generating Wake Word Samples for '{target_word}' (Advanced: 50k) into {WW_SAMPLES_OUTPUT_DIR_ADV} ---\")\n",
    "        \n",
    "        # Check if wake word samples already exist (asset caching)\n",
    "        assets_exist, file_count = check_assets_exist(WW_SAMPLES_OUTPUT_DIR_ADV, \"*.wav\")\n",
    "        if assets_exist:\n",
    "            print(f\"Found {file_count} existing wake word samples in {WW_SAMPLES_OUTPUT_DIR_ADV}\")\n",
    "            print(\"Skipping generation of new samples. Delete the directory if you want to regenerate.\")\n",
    "        else:\n",
    "            # Start here when trying to improve your model.\n",
    "            # See https://github.com/kiwina/piper-sample-generator for the full set of\n",
    "            # parameters. In particular, experiment with noise-scales and noise-scale-ws,\n",
    "            # generating negative samples similar to the wake word, and generating many more\n",
    "            # wake word samples, possibly with different phonetic pronunciations.\n",
    "            \n",
    "            ww_samples_cmd = f\"\\\"{sys.executable}\\\" \\\"{str(PIPER_SCRIPT_PATH)}\\\" \\\"{target_word}\\\" \\\\\n",
    "            --max-samples 50000 \\\\\n",
    "            --batch-size 100 \\\\\n",
    "            --output-dir \\\"{str(WW_SAMPLES_OUTPUT_DIR_ADV)}\\\"\"\n",
    "\n",
    "            print(f\"Executing: {ww_samples_cmd}\")\n",
    "            try:\n",
    "                if \"get_ipython\" in globals():\n",
    "                    get_ipython().system(ww_samples_cmd)\n",
    "                else:\n",
    "                    print(\"Warning: get_ipython() not available. Cannot execute command directly. For .py, use subprocess.\")\n",
    "                    # import subprocess\n",
    "                    # subprocess.run(ww_samples_cmd, shell=True, check=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing sample generation command: {e}\")\n",
    "                print(\"This might be due to missing dependencies or configuration issues.\")\n",
    "\n",
    "            print(f\"Wake word samples generation command issued for {WW_SAMPLES_OUTPUT_DIR_ADV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad24a37",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CELL 4: Sets up the augmentations (Advanced)\n",
    "# This cell assumes all necessary datasets (MIT RIR, Audioset, FMA)\n",
    "# have been downloaded and processed into their respective /data/..._16k directories\n",
    "# by the prepare_local_data.py script called in cell_2.py.\n",
    "\n",
    "import os\n",
    "from pathlib import Path # Ensure Path is imported\n",
    "from microwakeword.audio.augmentation import Augmentation\n",
    "from microwakeword.audio.clips import Clips\n",
    "# SpectrogramGeneration is imported in a later cell where it's used.\n",
    "\n",
    "# These variables should be defined in cell_2.py and available globally:\n",
    "# WW_SAMPLES_OUTPUT_DIR_ADV, MIT_RIRS_PATH, FMA_16K_PATH, AUDIOSONET_16K_PATH\n",
    "\n",
    "print(\"\\n--- Setting up Augmentations (Advanced) ---\")\n",
    "\n",
    "# Check if essential path variables from cell_2 exist\n",
    "required_paths_for_cell_4_adv = {\n",
    "    \"WW_SAMPLES_OUTPUT_DIR_ADV\": WW_SAMPLES_OUTPUT_DIR_ADV if 'WW_SAMPLES_OUTPUT_DIR_ADV' in globals() else None,\n",
    "    \"MIT_RIRS_PATH\": MIT_RIRS_PATH if 'MIT_RIRS_PATH' in globals() else None,\n",
    "    \"FMA_16K_PATH\": FMA_16K_PATH if 'FMA_16K_PATH' in globals() else None,\n",
    "    \"AUDIOSONET_16K_PATH\": AUDIOSONET_16K_PATH if 'AUDIOSONET_16K_PATH' in globals() else None\n",
    "}\n",
    "\n",
    "missing_paths_adv = [name for name, path_val in required_paths_for_cell_4_adv.items() if path_val is None or not Path(path_val).exists()]\n",
    "\n",
    "if missing_paths_adv:\n",
    "    print(f\"ERROR: One or more required data paths are missing or not defined from cell_2: {', '.join(missing_paths_adv)}\")\n",
    "    print(\"Please ensure cell_2.py (data preparation) ran successfully and all paths are correct.\")\n",
    "    clips_adv = None # Set to None to indicate failure to subsequent cells\n",
    "    augmenter_adv = None\n",
    "else:\n",
    "    print(f\"Using wake word samples from: {WW_SAMPLES_OUTPUT_DIR_ADV}\")\n",
    "    print(f\"Using MIT RIRs from: {MIT_RIRS_PATH}\")\n",
    "    print(f\"Using FMA 16k from: {FMA_16K_PATH}\")\n",
    "    print(f\"Using Audioset 16k from: {AUDIOSONET_16K_PATH}\")\n",
    "\n",
    "    clips_adv = Clips(\n",
    "        input_directory=str(WW_SAMPLES_OUTPUT_DIR_ADV), # From cell_2\n",
    "        file_pattern='*.wav',\n",
    "        max_clip_duration_s=5, # Advanced notebook might use a specific duration\n",
    "        remove_silence=True,   # Advanced notebook might enable silence removal\n",
    "        random_split_seed=10,\n",
    "        split_count=0.1, # Corresponds to 10% for test and 10% for validation\n",
    "    )\n",
    "\n",
    "    augmenter_adv = Augmentation(\n",
    "        augmentation_duration_s=3.2,\n",
    "        augmentation_probabilities={ # Example probabilities, can be tuned\n",
    "            \"SevenBandParametricEQ\": 0.1,\n",
    "            \"TanhDistortion\": 0.05, # Less distortion than basic\n",
    "            \"PitchShift\": 0.15,    # More pitch variation\n",
    "            \"BandStopFilter\": 0.1,\n",
    "            \"AddColorNoise\": 0.1,\n",
    "            \"AddBackgroundNoise\": 0.7, # Slightly less than basic, assuming cleaner negatives\n",
    "            \"Gain\": 0.8, # Allow more gain variation\n",
    "            \"RIR\": 0.7,  # More reverberation\n",
    "        },\n",
    "        impulse_paths=[str(MIT_RIRS_PATH)], # From cell_2\n",
    "        background_paths=[str(FMA_16K_PATH), str(AUDIOSONET_16K_PATH)], # From cell_2\n",
    "        background_min_snr_db=5,  # Higher min SNR for advanced\n",
    "        background_max_snr_db=15, # Higher max SNR\n",
    "        min_jitter_s=0.15, # Adjusted jitter\n",
    "        max_jitter_s=0.25,\n",
    "    )\n",
    "    print(\"Advanced augmentation setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67f81c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CELL 5: Augment a random clip and play it back to verify it works well (Advanced)\n",
    "from pathlib import Path # Ensure Path is imported\n",
    "\n",
    "# Conditional import for display\n",
    "try:\n",
    "    from IPython.display import Audio, display\n",
    "except ImportError:\n",
    "    def display(*args, **kwargs): pass\n",
    "    def Audio(*args, **kwargs): pass\n",
    "\n",
    "from microwakeword.audio.audio_utils import save_clip # Assuming this is installed with microwakeword\n",
    "\n",
    "# These variables should be defined from cell_2.py and cell_4.py:\n",
    "# DATA_DIR_INSIDE_DOCKER, clips_adv, augmenter_adv\n",
    "\n",
    "print(\"\\n--- Augmenting and Playing Test Clip (Advanced) ---\")\n",
    "\n",
    "if 'DATA_DIR_INSIDE_DOCKER' not in globals() or \\\n",
    "   'clips_adv' not in globals() or \\\n",
    "   'augmenter_adv' not in globals() or \\\n",
    "   clips_adv is None or \\\n",
    "   augmenter_adv is None:\n",
    "    print(\"ERROR: Essential variables (DATA_DIR_INSIDE_DOCKER, clips_adv, augmenter_adv) not defined or not initialized. Ensure cell_2.py and cell_4.py ran correctly.\")\n",
    "else:\n",
    "    augmented_clip_path_test_adv = DATA_DIR_INSIDE_DOCKER / \"augmented_clip_test_adv.wav\" # Save to /data root\n",
    "\n",
    "    try:\n",
    "        random_clip_data_adv = clips_adv.get_random_clip()\n",
    "        if random_clip_data_adv is None or random_clip_data_adv.size == 0:\n",
    "            print(\"ERROR: get_random_clip() returned None or empty data. Check Clips setup in cell_4 and sample generation in cell_3.\")\n",
    "        else:\n",
    "            augmented_clip_data_adv = augmenter_adv.augment_clip(random_clip_data_adv)\n",
    "            save_clip(augmented_clip_data_adv, str(augmented_clip_path_test_adv))\n",
    "            print(f\"Playing augmented test clip: {augmented_clip_path_test_adv}\")\n",
    "            display(Audio(str(augmented_clip_path_test_adv), autoplay=True))\n",
    "    except Exception as e:\n",
    "        print(f\"Error during advanced test augmentation: {e}. Check if previous cells ran successfully and data paths are correct.\")\n",
    "        print(\"Make sure 'clips_adv' and 'augmenter_adv' objects were created successfully in cell_4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43569ddd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CELL 6: Augment samples and save the training, validation, and testing sets (Advanced)\n",
    "import os\n",
    "from pathlib import Path # Ensure Path is imported\n",
    "from mmap_ninja.ragged import RaggedMmap\n",
    "from microwakeword.audio.spectrograms import SpectrogramGeneration # Ensure this is imported\n",
    "\n",
    "# These variables should be defined from cell_2.py and cell_4.py:\n",
    "# AUGMENTED_FEATURES_DIR_ADV, clips_adv, augmenter_adv\n",
    "\n",
    "print(\"\\n--- Generating Augmented Features for Training/Validation/Testing (Advanced) ---\")\n",
    "\n",
    "if 'AUGMENTED_FEATURES_DIR_ADV' not in globals() or \\\n",
    "   'clips_adv' not in globals() or \\\n",
    "   'augmenter_adv' not in globals() or \\\n",
    "   clips_adv is None or \\\n",
    "   augmenter_adv is None:\n",
    "    print(\"ERROR: Essential variables (AUGMENTED_FEATURES_DIR_ADV, clips_adv, augmenter_adv) not defined or not initialized. Ensure cell_2.py and cell_4.py ran correctly.\")\n",
    "else:\n",
    "    AUGMENTED_FEATURES_DIR_ADV.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    splits_config_adv = {\n",
    "        \"training\": {\"name\": \"train\", \"repetition\": 2, \"slide_frames\": 10}, # Default for basic, can be tuned for advanced\n",
    "        \"validation\": {\"name\": \"validation\", \"repetition\": 1, \"slide_frames\": 10},\n",
    "        \"testing\": {\"name\": \"test\", \"repetition\": 1, \"slide_frames\": 1}, # Streaming test\n",
    "    }\n",
    "\n",
    "    for split_item, config_adv in splits_config_adv.items():\n",
    "      out_dir_split_adv = AUGMENTED_FEATURES_DIR_ADV / split_item # Use variable from cell_2\n",
    "      out_dir_split_adv.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "      # Ensure clips_adv and augmenter_adv are not None (i.e., cell_4 executed successfully)\n",
    "      if clips_adv is None or augmenter_adv is None:\n",
    "          print(f\"Skipping feature generation for {split_item} due to missing clips_adv or augmenter_adv setup from cell_4.\")\n",
    "          continue\n",
    "\n",
    "      current_spectrograms_adv = SpectrogramGeneration(\n",
    "          clips=clips_adv,\n",
    "          augmenter=augmenter_adv,\n",
    "          slide_frames=config_adv[\"slide_frames\"],\n",
    "          step_ms=10, # Can be tuned\n",
    "      )\n",
    "\n",
    "      print(f\"Generating augmented features for {config_adv['name']} set into {out_dir_split_adv} (Advanced)...\")\n",
    "      try:\n",
    "        RaggedMmap.from_generator(\n",
    "            out_dir=str(out_dir_split_adv / 'wakeword_mmap'), # Ensure path is string\n",
    "            sample_generator=current_spectrograms_adv.spectrogram_generator(split=config_adv[\"name\"], repeat=config_adv[\"repetition\"]),\n",
    "            batch_size=100, # Can be tuned\n",
    "            verbose=True,\n",
    "        )\n",
    "        print(f\"Finished generating features for {config_adv['name']} set (Advanced).\")\n",
    "      except Exception as e:\n",
    "        print(f\"Error generating features for {config_adv['name']} set (Advanced): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb7de3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CELL 7: Save a yaml config that controls the training process (Advanced)\n",
    "# These hyperparamters can make a huge different in model quality.\n",
    "# Experiment with sampling and penalty weights and increasing the number of\n",
    "# training steps.\n",
    "import yaml\n",
    "import os\n",
    "from pathlib import Path # Ensure Path is imported\n",
    "\n",
    "# These variables should be defined in cell_2.py and available globally:\n",
    "# AUGMENTED_FEATURES_DIR_ADV, NEGATIVE_DATASETS_PATH,\n",
    "# TRAINED_MODELS_BASE_PATH_ADV, TRAINING_CONFIG_PATH_ADV\n",
    "\n",
    "print(\"\\n--- Preparing Training Configuration (Advanced) ---\")\n",
    "\n",
    "if 'AUGMENTED_FEATURES_DIR_ADV' not in globals() or \\\n",
    "   'NEGATIVE_DATASETS_PATH' not in globals() or \\\n",
    "   'TRAINED_MODELS_BASE_PATH_ADV' not in globals() or \\\n",
    "   'TRAINING_CONFIG_PATH_ADV' not in globals():\n",
    "    print(\"ERROR: Essential path variables for training config are not defined. Ensure cell_2.py ran correctly.\")\n",
    "    # Optionally, raise an error or sys.exit()\n",
    "    config_train_adv = {} # Create empty config to avoid further errors\n",
    "else:\n",
    "    # Ensure the base training directory exists\n",
    "    (TRAINED_MODELS_BASE_PATH_ADV / \"wakeword\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    config_train_adv = {\n",
    "        \"window_step_ms\": 10,\n",
    "        \"train_dir\": str(TRAINED_MODELS_BASE_PATH_ADV / \"wakeword\"), # Use variable\n",
    "        \"features\": [\n",
    "            {\n",
    "                \"features_dir\": str(AUGMENTED_FEATURES_DIR_ADV), # Use variable\n",
    "                \"sampling_weight\": 2.0, # Increased for advanced\n",
    "                \"penalty_weight\": 1.0,\n",
    "                \"truth\": True,\n",
    "                \"truncation_strategy\": \"truncate_start\",\n",
    "                \"type\": \"mmap\",\n",
    "            },\n",
    "            {\n",
    "                \"features_dir\": str(NEGATIVE_DATASETS_PATH / \"speech\"), # Use variable\n",
    "                \"sampling_weight\": 12.0, # Adjusted for advanced\n",
    "                \"penalty_weight\": 1.0,\n",
    "                \"truth\": False,\n",
    "                \"truncation_strategy\": \"random\",\n",
    "                \"type\": \"mmap\",\n",
    "            },\n",
    "            {\n",
    "                \"features_dir\": str(NEGATIVE_DATASETS_PATH / \"dinner_party\"), # Use variable\n",
    "                \"sampling_weight\": 12.0, # Adjusted for advanced\n",
    "                \"penalty_weight\": 1.0,\n",
    "                \"truth\": False,\n",
    "                \"truncation_strategy\": \"random\",\n",
    "                \"type\": \"mmap\",\n",
    "            },\n",
    "            {\n",
    "                \"features_dir\": str(NEGATIVE_DATASETS_PATH / \"no_speech\"), # Use variable\n",
    "                \"sampling_weight\": 5.0, # Balanced\n",
    "                \"penalty_weight\": 1.0,\n",
    "                \"truth\": False,\n",
    "                \"truncation_strategy\": \"random\",\n",
    "                \"type\": \"mmap\",\n",
    "            },\n",
    "            { # Only used for validation and testing\n",
    "                \"features_dir\": str(NEGATIVE_DATASETS_PATH / \"dinner_party_eval\"), # Use variable\n",
    "                \"sampling_weight\": 0.0,\n",
    "                \"penalty_weight\": 1.0,\n",
    "                \"truth\": False,\n",
    "                \"truncation_strategy\": \"split\",\n",
    "                \"type\": \"mmap\",\n",
    "            },\n",
    "        ],\n",
    "        \"training_steps\": [40000],  # Increased for advanced\n",
    "        \"positive_class_weight\": [1],\n",
    "        \"negative_class_weight\": [20],  # Adjusted\n",
    "        \"learning_rates\": [0.0005],  # Adjusted for advanced (potentially smaller LR for longer training)\n",
    "        \"batch_size\": 128,\n",
    "        \"time_mask_max_size\": [5],  # Enabled SpecAugment for advanced\n",
    "        \"time_mask_count\": [2],\n",
    "        \"freq_mask_max_size\": [5],\n",
    "        \"freq_mask_count\": [2],\n",
    "        \"eval_step_interval\": 1000,  # Evaluate less frequently for longer training\n",
    "        \"clip_duration_ms\": 2000,  # Potentially longer clips for advanced\n",
    "        \"target_minimization\": 0.85, # Stricter target for advanced\n",
    "        \"minimization_metric\": \"false_positive_rate\", # Example: target low FP rate\n",
    "        \"maximization_metric\": \"recall\" # Then maximize recall\n",
    "    }\n",
    "\n",
    "    with open(TRAINING_CONFIG_PATH_ADV, \"w\") as file_yaml_adv: # Use variable\n",
    "        yaml.dump(config_train_adv, file_yaml_adv)\n",
    "    print(f\"Advanced training parameters saved to {TRAINING_CONFIG_PATH_ADV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307cfaa6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CELL 8: Trains a model (Advanced)\n",
    "# This cell executes the main model training process using the configuration\n",
    "# defined in the previous cell and saved to training_parameters_adv.yaml.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path # Ensure Path is imported\n",
    "\n",
    "# TRAINING_CONFIG_PATH_ADV should be defined in cell_2.py and the YAML file created in cell_7.py\n",
    "# Example: TRAINING_CONFIG_PATH_ADV = DATA_DIR_INSIDE_DOCKER / \"training_parameters_adv.yaml\"\n",
    "\n",
    "print(\"\\n--- Starting Model Training (Advanced) ---\")\n",
    "\n",
    "if 'TRAINING_CONFIG_PATH_ADV' not in globals() or not TRAINING_CONFIG_PATH_ADV.exists():\n",
    "    print(f\"ERROR: Training config path {TRAINING_CONFIG_PATH_ADV if 'TRAINING_CONFIG_PATH_ADV' in globals() else 'TRAINING_CONFIG_PATH_ADV variable not found'} not found or variable not defined. Ensure cell_2.py and cell_7.py ran correctly.\")\n",
    "else:\n",
    "    print(f\"Starting advanced model training using config: {TRAINING_CONFIG_PATH_ADV}\")\n",
    "\n",
    "    # Construct the command string for the ! operator\n",
    "    # Using more advanced/deeper model parameters as an example\n",
    "    training_command_adv = f\"\\\"{sys.executable}\\\" -m microwakeword.model_train_eval \\\\\n",
    "    --training_config='{str(TRAINING_CONFIG_PATH_ADV)}' \\\\\n",
    "    --train 1 \\\\\n",
    "    --restore_checkpoint 1 \\\\\n",
    "    --test_tf_nonstreaming 0 \\\\\n",
    "    --test_tflite_nonstreaming 0 \\\\\n",
    "    --test_tflite_nonstreaming_quantized 0 \\\\\n",
    "    --test_tflite_streaming 0 \\\\\n",
    "    --test_tflite_streaming_quantized 1 \\\\\n",
    "    --use_weights \\\"best_weights\\\" \\\\\n",
    "    mixednet \\\\\n",
    "    --pointwise_filters \\\"64,64,64,64,64\\\" \\\\\n",
    "    --repeat_in_block  \\\"1,1,1,1,1\\\" \\\\\n",
    "    --mixconv_kernel_sizes '[3,5],[5,7],[7,9],[9,11],[11,13]' \\\\\n",
    "    --residual_connection \\\"0,0,0,0,0\\\" \\\\\n",
    "    --first_conv_filters 48 \\\\\n",
    "    --first_conv_kernel_size 7 \\\\\n",
    "    --stride 2\" # Example: different first layer, stride\n",
    "\n",
    "    print(f\"Executing advanced training command:\\n{training_command_adv}\")\n",
    "\n",
    "    if \"get_ipython\" in globals():\n",
    "        get_ipython().system(training_command_adv)\n",
    "    else:\n",
    "        print(\"Warning: get_ipython() not available. Cannot execute training command directly in this .py script.\")\n",
    "        print(\"This cell is intended to be converted back to an .ipynb cell.\")\n",
    "        # As a fallback for pure .py script testing:\n",
    "        # import subprocess\n",
    "        # subprocess.run(training_command_adv, shell=True, check=True)\n",
    "\n",
    "print(\"Advanced model training/evaluation finished (or command printed if not in IPython).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f819a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CELL 9: Prepare Output Model Files (Advanced)\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path # Ensure Path is imported\n",
    "\n",
    "# Conditional import for display\n",
    "try:\n",
    "    from IPython.display import FileLink, display\n",
    "except ImportError:\n",
    "    def display(*args, **kwargs): pass\n",
    "    def FileLink(*args, **kwargs): pass\n",
    "\n",
    "# These variables should be defined in cell_2.py:\n",
    "# target_word, TRAINED_MODELS_BASE_PATH_ADV, DATA_DIR_INSIDE_DOCKER\n",
    "\n",
    "print(\"\\n--- Preparing Output Model Files (Advanced) ---\")\n",
    "\n",
    "if 'target_word' not in globals() or \\\n",
    "   'TRAINED_MODELS_BASE_PATH_ADV' not in globals() or \\\n",
    "   'DATA_DIR_INSIDE_DOCKER' not in globals():\n",
    "    print(\"ERROR: Essential variables (target_word, TRAINED_MODELS_BASE_PATH_ADV, DATA_DIR_INSIDE_DOCKER) not defined. Ensure cell_2.py ran correctly.\")\n",
    "else:\n",
    "    source_tflite_path_adv = TRAINED_MODELS_BASE_PATH_ADV / \"wakeword/tflite_stream_state_internal_quant/stream_state_internal_quant.tflite\"\n",
    "    # Save final model files directly into the /data root for easy access from host, with _adv suffix\n",
    "    destination_tflite_path_adv = DATA_DIR_INSIDE_DOCKER / f\"{target_word}_advanced_model.tflite\"\n",
    "    destination_json_path_adv = DATA_DIR_INSIDE_DOCKER / f\"{target_word}_advanced_model.json\"\n",
    "\n",
    "    if source_tflite_path_adv.exists():\n",
    "        shutil.copy(source_tflite_path_adv, destination_tflite_path_adv)\n",
    "        print(f\"Copied ADVANCED TFLite model to {destination_tflite_path_adv}\")\n",
    "    else:\n",
    "        print(f\"ERROR: Trained ADVANCED TFLite model not found at {source_tflite_path_adv}. Training might have failed or model path is incorrect.\")\n",
    "\n",
    "    json_data_adv = {\n",
    "        \"type\": \"micro\",\n",
    "        \"wake_word\": target_word,  # Using the target_word variable from cell_2\n",
    "        \"author\": \"kiwina\", \n",
    "        \"website\": \"https://github.com/kiwina/MicroWakeWord-Trainer-Docker\",\n",
    "        \"model\": f\"{target_word}_advanced_model.tflite\", # Relative path for use with ESPHome\n",
    "        \"trained_languages\": [\"en\"],\n",
    "        \"version\": 1, # Start version at 1 for a new advanced model, user can increment\n",
    "        \"micro\": { # Example parameters for an \"advanced\" model, tune as needed\n",
    "            \"probability_cutoff\": 0.90, # Potentially lower for better recall if FP is managed\n",
    "            \"sliding_window_size\": 6,   # Might be larger/smaller depending on model architecture\n",
    "            \"feature_step_size\": 10,\n",
    "            \"tensor_arena_size\": 35000, # Potentially larger for a more complex model\n",
    "            \"minimum_esphome_version\": \"2024.7.0\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(destination_json_path_adv, \"w\") as json_file_adv:\n",
    "        json.dump(json_data_adv, json_file_adv, indent=2)\n",
    "    print(f\"Created ADVANCED JSON metadata at {destination_json_path_adv}\")\n",
    "\n",
    "    print(\"\\n--- Advanced Script Finished ---\")\n",
    "    print(f\"Output files for ADVANCED model are located in: {DATA_DIR_INSIDE_DOCKER.resolve().absolute()}\")\n",
    "    print(\"If running in Docker, this corresponds to the 'microwakeword-trainer-data' directory on your host machine.\")\n",
    "\n",
    "    if destination_tflite_path_adv.exists():\n",
    "        print(\"\\nADVANCED TFLite Model Link (for Jupyter environments):\")\n",
    "        display(FileLink(str(destination_tflite_path_adv)))\n",
    "    if destination_json_path_adv.exists():\n",
    "        print(\"\\nADVANCED JSON Metadata Link (for Jupyter environments):\")\n",
    "        display(FileLink(str(destination_json_path_adv)))\n",
    "\n",
    "# Cells 10, 11, 12 from the original advanced notebook are now effectively covered or made redundant\n",
    "# by the new structure (cell_7 for YAML, cell_8 for training, this cell_9 for output).\n",
    "# If they had other unique logic, it would need to be merged or placed in a new cell.\n",
    "# For now, we assume this cell_9 is the last meaningful code cell for the advanced notebook.\n",
    "\n",
    "\n",
    "# Cells 10 and 11 from the original advanced notebook have been removed as they are redundant\n",
    "# Cell 10 duplicated the training functionality already in cell 8\n",
    "# Cell 11 duplicated the model file preparation already in cell 9"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
