{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "  <img src=\"https://raw.githubusercontent.com/MasterPhooey/MicroWakeWord-Trainer-Docker/refs/heads/main/mmw.png\" alt=\"MicroWakeWord Trainer Logo\" width=\"100\" />\n",
        "\n",
        "  <h1>MicroWakeWord Trainer Docker</h1>\n",
        "\n",
        "</div>\n",
        "\n",
        "This notebook steps you through training a basic microWakeWord model. It is intended as a **starting point** for advanced users. You should use Python 3.11.\n",
        "\n",
        "**The model generated will most likely not be usable for everyday use; it may be difficult to trigger or falsely activates too frequently. You will most likely have to experiment with many different settings to obtain a decent model!**\n",
        "\n",
        "In the comment at the start of certain blocks, I note some specific settings to consider modifying.\n",
        "\n",
        "This runs on Google Colab, but is extremely slow compared to training on a local GPU. If you must use Colab, be sure to Change the runtime type to a GPU. Even then, it still slow!\n",
        "\n",
        "At the end of this notebook, you will be able to download a tflite file. To use this in ESPHome, you need to write a model manifest JSON file. See the [ESPHome documentation](https://esphome.io/components/micro_wake_word) for the details and the [model repo](https://github.com/esphome/micro-wake-word-models/tree/main/models/v2) for examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Initial Setup and Data Preparation\n",
        "# This cell runs the main data preparation script.\n",
        "# It ensures all necessary repositories and datasets are downloaded and processed into /data.\n",
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"Starting data preparation...\")\n",
        "# Assuming prepare_local_data.py is in /data, and this notebook runs from /data as well.\n",
        "# The --data-dir /data ensures the script uses the correct base path inside Docker.\n",
        "prepare_script_path = \"/data/prepare_local_data.py\"\n",
        "if not os.path.exists(prepare_script_path):\n",
        "    # Fallback if script is in parent dir relative to /data (e.g. /prepare_local_data.py)\n",
        "    prepare_script_path = \"../prepare_local_data.py\" \n",
        "    if not os.path.exists(prepare_script_path):\n",
        "        print(f\"ERROR: {prepare_script_path} not found. Cannot prepare data.\")\n",
        "        # Consider raising an exception or sys.exit(1)\n",
        "else:\n",
        "    print(f\"Executing: !python {prepare_script_path} --data-dir /data\")\n",
        "    !python {prepare_script_path} --data-dir /data\n",
        "print(\"Data preparation script finished.\")\n",
        "\n",
        "# Ensure piper-sample-generator is in sys.path if it's used as a collection of scripts\n",
        "piper_path = \"/data/piper-sample-generator\"\n",
        "if piper_path not in sys.path:\n",
        "    sys.path.append(piper_path)\n",
        "    print(f\"Added {piper_path} to sys.path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Generates 1 sample of the target word for manual verification.\n",
        "target_word = 'khum_puter'  # Phonetic spellings may produce better samples\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "piper_script_path = \"/data/piper-sample-generator/generate_samples.py\"\n",
        "output_sample_dir = \"/data/generated_samples_test\"\n",
        "os.makedirs(output_sample_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(piper_script_path):\n",
        "    print(f\"ERROR: Piper sample generator script not found at {piper_script_path}. Check data preparation.\")\n",
        "else:\n",
        "    print(f\"Generating test sample for '{target_word}'...\")\n",
        "    !\"{sys.executable}\" {piper_script_path} \"{target_word}\" \\\n",
        "    --max-samples 1 \\\n",
        "    --batch-size 1 \\\n",
        "    --output-dir {output_sample_dir}\n",
        "\n",
        "    # Play the generated audio sample\n",
        "    audio_path = os.path.join(output_sample_dir, \"0.wav\")\n",
        "    if os.path.exists(audio_path):\n",
        "        print(f\"Playing test sample: {audio_path}\")\n",
        "        display(Audio(audio_path, autoplay=True))\n",
        "    else:\n",
        "        print(f\"Audio file not found at {audio_path}. Sample generation might have failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Generates a larger amount of wake word samples.\n",
        "# Start here when trying to improve your model.\n",
        "# See https://github.com/rhasspy/piper-sample-generator for the full set of\n",
        "# parameters. In particular, experiment with noise-scales and noise-scale-ws,\n",
        "# generating negative samples similar to the wake word, and generating many more\n",
        "# wake word samples, possibly with different phonetic pronunciations.\n",
        "import os\n",
        "import sys\n",
        "\n",
        "piper_script_path = \"/data/piper-sample-generator/generate_samples.py\"\n",
        "output_ww_dir = \"/data/generated_samples_ww\"\n",
        "os.makedirs(output_ww_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(piper_script_path):\n",
        "    print(f\"ERROR: Piper sample generator script not found at {piper_script_path}. Check data preparation.\")\n",
        "else:\n",
        "    print(f\"Generating wake word samples for '{target_word}'...\")\n",
        "    !\"{sys.executable}\" {piper_script_path} \"{target_word}\" \\\n",
        "    --max-samples 1000 \\\n",
        "    --batch-size 100 \\\n",
        "    --output-dir {output_ww_dir}\n",
        "    print(f\"Wake word samples generated in {output_ww_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augmentation Data Setup\n",
        "The `prepare_local_data.py` script should have downloaded and processed all necessary augmentation data (MIT RIR, Audioset, FMA) into subdirectories within `/data`.\n",
        "The following cells will set up the augmentation using these pre-prepared datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Sets up the augmentations.\n",
        "# To improve your model, experiment with these settings and use more sources of\n",
        "# background clips.\n",
        "\n",
        "from microwakeword.audio.augmentation import Augmentation\n",
        "from microwakeword.audio.clips import Clips\n",
        "from microwakeword.audio.spectrograms import SpectrogramGeneration\n",
        "import os\n",
        "\n",
        "# Paths to pre-prepared data in /data directory\n",
        "generated_samples_path = \"/data/generated_samples_ww\" # From previous cell\n",
        "mit_rirs_path = \"/data/mit_rirs\"\n",
        "fma_16k_path = \"/data/fma_16k\"\n",
        "audioset_16k_path = \"/data/audioset_16k\"\n",
        "\n",
        "if not os.path.exists(generated_samples_path):\n",
        "    print(f\"ERROR: Wake word samples not found at {generated_samples_path}. Please run the previous cell.\")\n",
        "if not os.path.exists(mit_rirs_path) or not os.path.exists(fma_16k_path) or not os.path.exists(audioset_16k_path):\n",
        "    print(\"ERROR: One or more augmentation data directories (MIT RIRs, FMA, Audioset) not found in /data. Please ensure prepare_local_data.py ran successfully.\")\n",
        "\n",
        "clips = Clips(input_directory=generated_samples_path,\n",
        "              file_pattern='*.wav',\n",
        "              max_clip_duration_s=None,\n",
        "              remove_silence=False,\n",
        "              random_split_seed=10,\n",
        "              split_count=0.1,\n",
        "             )\n",
        "\n",
        "augmenter = Augmentation(augmentation_duration_s=3.2,\n",
        "                         augmentation_probabilities = {\n",
        "                                \"SevenBandParametricEQ\": 0.1,\n",
        "                                \"TanhDistortion\": 0.1,\n",
        "                                \"PitchShift\": 0.1,\n",
        "                                \"BandStopFilter\": 0.1,\n",
        "                                \"AddColorNoise\": 0.1,\n",
        "                                \"AddBackgroundNoise\": 0.75,\n",
        "                                \"Gain\": 1.0,\n",
        "                                \"RIR\": 0.5,\n",
        "                            },\n",
        "                         impulse_paths = [mit_rirs_path],\n",
        "                         background_paths = [fma_16k_path, audioset_16k_path],\n",
        "                         background_min_snr_db = -5,\n",
        "                         background_max_snr_db = 10,\n",
        "                         min_jitter_s = 0.195,\n",
        "                         max_jitter_s = 0.205,\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Augment a random clip and play it back to verify it works well\n",
        "from IPython.display import Audio, display\n",
        "from microwakeword.audio.audio_utils import save_clip\n",
        "import os\n",
        "\n",
        "augmented_clip_path = \"/data/augmented_clip_test.wav\"\n",
        "\n",
        "try:\n",
        "    random_clip = clips.get_random_clip()\n",
        "    augmented_clip = augmenter.augment_clip(random_clip)\n",
        "    save_clip(augmented_clip, augmented_clip_path)\n",
        "    print(f\"Playing augmented test clip: {augmented_clip_path}\")\n",
        "    display(Audio(augmented_clip_path, autoplay=True))\n",
        "except Exception as e:\n",
        "    print(f\"Error during test augmentation: {e}. Check if previous cells ran successfully and data paths are correct.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Augment samples and save the training, validation, and testing sets.\n",
        "# Validating and testing samples generated the same way can make the model\n",
        "# benchmark better than it performs in real-word use. Use real samples or TTS\n",
        "# samples generated with a different TTS engine to potentially get more accurate\n",
        "# benchmarks.\n",
        "import os\n",
        "from mmap_ninja.ragged import RaggedMmap\n",
        "\n",
        "output_dir_augmented_features = '/data/generated_augmented_features'\n",
        "os.makedirs(output_dir_augmented_features, exist_ok=True)\n",
        "\n",
        "splits = [\"training\", \"validation\", \"testing\"]\n",
        "for split in splits:\n",
        "  out_dir_split = os.path.join(output_dir_augmented_features, split)\n",
        "  os.makedirs(out_dir_split, exist_ok=True)\n",
        "\n",
        "  split_name = \"train\"\n",
        "  repetition = 2\n",
        "\n",
        "  spectrograms = SpectrogramGeneration(clips=clips,\n",
        "                                     augmenter=augmenter,\n",
        "                                     slide_frames=10,    # Uses the same spectrogram repeatedly, just shifted over by one frame. This simulates the streaming inferences while training/validating in nonstreaming mode.\n",
        "                                     step_ms=10,\n",
        "                                     )\n",
        "  if split == \"validation\":\n",
        "    split_name = \"validation\"\n",
        "    repetition = 1\n",
        "  elif split == \"testing\":\n",
        "    split_name = \"test\"\n",
        "    repetition = 1\n",
        "    spectrograms = SpectrogramGeneration(clips=clips,\n",
        "                                     augmenter=augmenter,\n",
        "                                     slide_frames=1,    # The testing set uses the streaming version of the model, so no artificial repetition is necessary\n",
        "                                     step_ms=10,\n",
        "                                     )\n",
        "\n",
        "  print(f\"Generating augmented features for {split_name} set...\")\n",
        "  try:\n",
        "    RaggedMmap.from_generator(\n",
        "        out_dir=os.path.join(out_dir_split, 'wakeword_mmap'),\n",
        "        sample_generator=spectrograms.spectrogram_generator(split=split_name, repeat=repetition),\n",
        "        batch_size=100,\n",
        "        verbose=True,\n",
        "    )\n",
        "    print(f\"Finished generating features for {split_name} set.\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error generating features for {split_name} set: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Negative Datasets Setup\n",
        "The `prepare_local_data.py` script should have downloaded and extracted pre-generated negative spectrogram features into `/data/negative_datasets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Save a yaml config that controls the training process\n",
        "# These hyperparamters can make a huge different in model quality.\n",
        "# Experiment with sampling and penalty weights and increasing the number of\n",
        "# training steps.\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "config = {}\n",
        "\n",
        "config[\"window_step_ms\"] = 10\n",
        "\n",
        "config[\"train_dir\"] = (\n",
        "    \"/data/trained_models/wakeword\" # Path inside Docker\n",
        ")\n",
        "os.makedirs(config[\"train_dir\"], exist_ok=True)\n",
        "\n",
        "# Each feature_dir should have at least one of the following folders with this structure:\n",
        "#  training/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  testing/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  testing_ambient/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  validation/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  validation_ambient/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#\n",
        "#  sampling_weight: Weight for choosing a spectrogram from this set in the batch\n",
        "#  penalty_weight: Penalizing weight for incorrect predictions from this set\n",
        "#  truth: Boolean whether this set has positive samples or negative samples\n",
        "#  truncation_strategy = If spectrograms in the set are longer than necessary for training, how are they truncated\n",
        "#       - random: choose a random portion of the entire spectrogram - useful for long negative samples\n",
        "#       - truncate_start: remove the start of the spectrogram\n",
        "#       - truncate_end: remove the end of the spectrogram\n",
        "#       - split: Split the longer spectrogram into separate spectrograms offset by 100 ms. Only for ambient sets\n",
        "\n",
        "config[\"features\"] = [\n",
        "    {\n",
        "        \"features_dir\": \"/data/generated_augmented_features\", # Path inside Docker\n",
        "        \"sampling_weight\": 2.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": True,\n",
        "        \"truncation_strategy\": \"truncate_start\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"/data/negative_datasets/speech\", # Path inside Docker\n",
        "        \"sampling_weight\": 10.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"/data/negative_datasets/dinner_party\", # Path inside Docker\n",
        "        \"sampling_weight\": 10.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"/data/negative_datasets/no_speech\", # Path inside Docker\n",
        "        \"sampling_weight\": 5.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    { # Only used for validation and testing\n",
        "        \"features_dir\": \"/data/negative_datasets/dinner_party_eval\", # Path inside Docker\n",
        "        \"sampling_weight\": 0.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"split\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Number of training steps in each iteration - various other settings are configured as lists that corresponds to different steps\n",
        "config[\"training_steps\"] = [10000]\n",
        "\n",
        "# Penalizing weight for incorrect class predictions - lists that correspond to training steps\n",
        "config[\"positive_class_weight\"] = [1]\n",
        "config[\"negative_class_weight\"] = [20]\n",
        "\n",
        "config[\"learning_rates\"] = [\n",
        "    0.001,\n",
        "]  # Learning rates for Adam optimizer - list that corresponds to training steps\n",
        "config[\"batch_size\"] = 128\n",
        "\n",
        "config[\"time_mask_max_size\"] = [\n",
        "    0\n",
        "]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"time_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"freq_mask_max_size\"] = [\n",
        "    0\n",
        "]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"freq_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
        "\n",
        "config[\"eval_step_interval\"] = (\n",
        "    500  # Test the validation sets after every this many steps\n",
        ")\n",
        "config[\"clip_duration_ms\"] = (\n",
        "    1500  # Maximum length of wake word that the streaming model will accept\n",
        ")\n",
        "\n",
        "# The best model weights are chosen first by minimizing the specified minimization metric below the specified target_minimization\n",
        "# Once the target has been met, it chooses the maximum of the maximization metric. Set 'minimization_metric' to None to only maximize\n",
        "# Available metrics:\n",
        "#   - \"loss\" - cross entropy error on validation set\n",
        "#   - \"accuracy\" - accuracy of validation set\n",
        "#   - \"recall\" - recall of validation set\n",
        "#   - \"precision\" - precision of validation set\n",
        "#   - \"false_positive_rate\" - false positive rate of validation set\n",
        "#   - \"false_negative_rate\" - false negative rate of validation set\n",
        "#   - \"ambient_false_positives\" - count of false positives from the split validation_ambient set\n",
        "#   - \"ambient_false_positives_per_hour\" - estimated number of false positives per hour on the split validation_ambient set\n",
        "config[\"target_minimization\"] = 0.9\n",
        "config[\"minimization_metric\"] = None  # Set to None to disable\n",
        "\n",
        "config[\"maximization_metric\"] = \"average_viable_recall\"\n",
        "\n",
        "training_params_path = \"/data/training_parameters.yaml\"\n",
        "with open(training_params_path, \"w\") as file:\n",
        "    documents = yaml.dump(config, file)\n",
        "print(f\"Training parameters saved to {training_params_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Trains a model. When finished, it will quantize and convert the model to a\n",
        "# streaming version suitable for on-device detection.\n",
        "# It will resume if stopped, but it will start over at the configured training\n",
        "# steps in the yaml file.\n",
        "# Change --train 0 to only convert and test the best-weighted model.\n",
        "# On Google colab, it doesn't print the mini-batch results, so it may appear\n",
        "# stuck for several minutes! Additionally, it is very slow compared to training\n",
        "# on a local GPU.\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# LD_LIBRARY_PATH might be needed if base TF image doesn't set it up for all custom ops, but usually it's fine.\n",
        "# os.environ['LD_LIBRARY_PATH'] = \"/usr/lib/x86_64-linux-gnu:\" + os.environ.get('LD_LIBRARY_PATH', '')\n",
        "\n",
        "training_params_path = \"/data/training_parameters.yaml\"\n",
        "print(f\"Starting model training using config: {training_params_path}\")\n",
        "\n",
        "!\"{sys.executable}\" -m microwakeword.model_train_eval \\\n",
        "    --training_config='{training_params_path}' \\\n",
        "    --train 1 \\\n",
        "    --restore_checkpoint 1 \\\n",
        "    --test_tf_nonstreaming 0 \\\n",
        "    --test_tflite_nonstreaming 0 \\\n",
        "    --test_tflite_nonstreaming_quantized 0 \\\n",
        "    --test_tflite_streaming 0 \\\n",
        "    --test_tflite_streaming_quantized 1 \\\n",
        "    --use_weights \"best_weights\" \\\n",
        "    mixednet \\\n",
        "    --pointwise_filters \"64,64,64,64\" \\\n",
        "    --repeat_in_block  \"1, 1, 1, 1\" \\\n",
        "    --mixconv_kernel_sizes '[5], [7,11], [9,15], [23]' \\\n",
        "    --residual_connection \"0,0,0,0\" \\\n",
        "    --first_conv_filters 32 \\\n",
        "    --first_conv_kernel_size 5 \\\n",
        "    --stride 3\n",
        "\n",
        "print(\"Model training/evaluation finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import json\n",
        "import os\n",
        "from IPython.display import FileLink, display\n",
        "\n",
        "# target_word should be defined in an earlier cell (e.g., where test sample is generated)\n",
        "# If not, define it here or ensure it's passed correctly.\n",
        "# For example: target_word = 'khum_puter' \n",
        "\n",
        "# Define the source path and desired download location for the TFLite file\n",
        "source_tflite_path = \"/data/trained_models/wakeword/tflite_stream_state_internal_quant/stream_state_internal_quant.tflite\"\n",
        "destination_tflite_path = \"/data/stream_state_internal_quant.tflite\" # Will be accessible from host via mapped /data volume\n",
        "\n",
        "if os.path.exists(source_tflite_path):\n",
        "    shutil.copy(source_tflite_path, destination_tflite_path)\n",
        "    print(f\"Copied TFLite model to {destination_tflite_path}\")\n",
        "else:\n",
        "    print(f\"ERROR: Trained TFLite model not found at {source_tflite_path}\")\n",
        "\n",
        "# Define the JSON file content\n",
        "json_data = {\n",
        "    \"type\": \"micro\",\n",
        "    \"wake_word\": target_word,  # Using the target_word variable\n",
        "    \"author\": \"master phooey\", # Or your name/handle\n",
        "    \"website\": \"https://github.com/kiwina/MicroWakeWord-Trainer-Docker\", # Updated to kiwina fork\n",
        "    \"model\": \"stream_state_internal_quant.tflite\", # Relative path for use with ESPHome\n",
        "    \"trained_languages\": [\"en\"],\n",
        "    \"version\": 2, # Increment if you retrain and improve\n",
        "    \"micro\": {\n",
        "        \"probability_cutoff\": 0.97, # Adjust based on testing\n",
        "        \"sliding_window_size\": 5,\n",
        "        \"feature_step_size\": 10,\n",
        "        \"tensor_arena_size\": 30000, # Adjust based on model needs\n",
        "        \"minimum_esphome_version\": \"2024.7.0\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define the JSON file path\n",
        "destination_json_path = \"/data/stream_state_internal_quant.json\"\n",
        "\n",
        "# Write the JSON file\n",
        "with open(destination_json_path, \"w\") as json_file:\n",
        "    json.dump(json_data, json_file, indent=2)\n",
        "print(f\"Created JSON metadata at {destination_json_path}\")\n",
        "\n",
        "# Generate download links for both files (if running in a Jupyter environment that supports this)\n",
        "print(\"\\nAccess your files in the 'microwakeword-trainer-data' directory on your host machine.\")\n",
        "if os.path.exists(destination_tflite_path):\n",
        "    print(\"TFLite Model Link (for Jupyter environments):\")\n",
        "    display(FileLink(destination_tflite_path))\n",
        "if os.path.exists(destination_json_path):\n",
        "    print(\"\\nJSON Metadata Link (for Jupyter environments):\")\n",
        "    display(FileLink(destination_json_path))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
